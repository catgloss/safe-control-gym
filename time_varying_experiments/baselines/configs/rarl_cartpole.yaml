# model args
algo_config: 
  hidden_dim: 64
  norm_obs: False
  norm_reward: False
  clip_obs: 10.
  clip_reward: 10.
  observation_space: None

  # loss args
  gamma: 0.99
  use_gae: False
  gae_lambda: 0.95
  use_clipped_value: False
  clip_param: 0.2
  target_kl: 0.01
  entropy_coef: 0.01

  # optim args
  opt_epochs: 10
  mini_batch_size: 64
  actor_lr: 0.0003
  critic_lr: 0.001
  max_grad_norm: 0.5
  agent_iterations: 10
  adversary_iterations: 10

  pretrained: null 
  train_protagonist: True 
  train_adversary: True 

  # runner args
  max_env_steps: 1000000
  num_workers: 1
  rollout_batch_size: 4
  rollout_steps: 100
  deque_size: 10
  eval_batch_size: 10

  # misc
  log_interval: 1000
  save_interval: 0
  num_checkpoints: 0
  eval_interval: 0
  eval_save_best: False 
  tensorboard: False

task_config:
  info_in_reset: True
  normalized_rl_action_space: False
  constraints:
    - constraint_form: abs_bound
      bound: 0.6
      constrained_variable: state
      active_dims: 0
    - constraint_form: abs_bound
      bound: 0.5
      constrained_variable: state
      active_dims: 2 
  done_on_violation: False
  randomized_init: False
  init_state: 
    init_x: 0.5 
    init_x_dot: 0.1
    init_theta: -0.15
    init_theta_dot: -0.1
  adversary_disturbance: dynamics 
  adversary_disturbance_offset: 0.0
  adversary_disturbance_scale: 0.01

