# model args
algo_config: 
  hidden_dim: 64
  norm_obs: False
  norm_reward: False
  clip_obs: 10.
  clip_reward: 10.
  observation_space: None

  # loss args
  gamma: 0.99
  use_gae: False
  gae_lambda: 0.95
  use_clipped_value: False
  clip_param: 0.2
  target_kl: 0.01
  entropy_coef: 0.01

  # optim args
  opt_epochs: 10
  mini_batch_size: 64
  actor_lr: 0.0003
  critic_lr: 0.001
  max_grad_norm: 0.5
  agent_iterations: 10
  adversary_iterations: 10

  pretrained: null 
  train_protagonist: True 
  train_adversary: True 

  # runner args
  max_env_steps: 1000000
  num_workers: 1
  rollout_batch_size: 4
  rollout_steps: 100
  deque_size: 10
  eval_batch_size: 10

  # misc
  log_interval: 1000
  save_interval: 0
  num_checkpoints: 0
  eval_interval: 0
  eval_save_best: False 
  tensorboard: False

render: true
task_config:
  info_in_reset: True
  normalized_rl_action_space: False
  constraints:
    - constraint_form: abs_bound
      bound: 1
      constrained_variable: state
      active_dims: 0
    - constraint_form: abs_bound
      bound: 0.5
      constrained_variable: state
      active_dims: 2 
  done_on_violation: False
  # init_state: 
  #   init_x: 0.55
  #   init_x_dot: 0.5
  #   init_theta: 0.30
  #   init_theta_dot: -0.1
  # randomized_init: False
  disturbances: 
    dynamics:
    - disturbance_func: white_noise
      std: 0.15
  init_state_randomization_info:
    init_x:
      distrib: 'uniform'
      low: -0.5
      high: 0.5
    init_x_dot:
      distrib: 'uniform'
      low: -0.5
      high: 0.5
    init_theta:
      distrib: 'uniform'
      low: -0.35
      high: 0.35
    init_theta_dot:
      distrib: 'uniform'
      low: -0.15
      high: 0.15
  # env_disturbance: 
  #   variable: dynamics
  #   scheduler: cyclic
  #   active_dims: [0,1]
  #   start: [1,0]
  #   sign: [-1,1]
  #   max: [1, 1]
  #   min: [0,-1]
  #   scale: [0.1, 0.1]
  #   std: [0.01, 0.01]
  adversary_disturbance: dynamics 
  adversary_disturbance_offset: 0.0
  adversary_disturbance_scale: 0.01

